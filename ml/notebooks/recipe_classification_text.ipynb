{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe Classification using text data\n",
    "\n",
    "In order to try this notebook, you need prepare your own data. <br>\n",
    "Directory structure is assumed as following (category0 or 1 can be replaced with the name of the category). <br>\n",
    "\n",
    "- /work/data/text/recipes\n",
    "  - /category0_negative.pkl\n",
    "  - /category0_positive.pkl\n",
    "  - /category1_negative.pkl\n",
    "  - /category1_positive.pkl\n",
    "  - ...\n",
    "\n",
    "Data format is assumed to be json format.\n",
    "\n",
    "The topic is the recipe classification using {title, ingredient, step} text data.\n",
    "\n",
    "We treat this problem as a binary classification {0: The target recipe,1: Not the target recipe}; we just construct classification models whose input is preprocessed text features.\n",
    "\n",
    "- **1. Prepare dataset** <br>\n",
    "- **2. Construct a Random Forest model** <br>\n",
    "- **3. Construct a Xgboost model** <br>\n",
    "- **4. How can we improve the model?** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of recipe data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    1: {'title': \"激ウマ！焼き餃子\", 'ingredients': [\"豚肉\",\"にんにく\",\"にら\"], 'steps': [\"たねを作る\",\"皮に包む\",\"美味しく焼く\"]}\n",
    "    , 2: {'title': \"最高！焼き餃子\", 'ingredients': [\"豚肉\",\"にんにく\",\"にら\",\"春菊\"], 'steps': [\"たねを作る\",\"皮に包む\",\"美味しく焼く\"]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepared a sample data to try this notebook as below.\n",
    "- */work/data/text/recipes/sample_positive.pkl* \n",
    "- */work/data/text/recipes/sample_negative.pkl* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare dataset\n",
    "\n",
    "In Japanese text analysis, we need to do morphological analysis in order to break down text to set of words.\n",
    "\n",
    "Here we use Mecab ( http://taku910.github.io/mecab/ ) for morphological analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import MeCab\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define signs which will be eliminated and load cookpad dictionary.\n",
    "\n",
    "We assume you have already downloaded the data from the s3 bucket. \n",
    "\n",
    "Various signs may become noises in modeling, so we omit signs.\n",
    "\n",
    "Since WE have the useful cookpad dictionary, we use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[reference] Check the behavior with the default dictionary.\n",
    "\n",
    "老干媽 is a kind of chilli oil. Recipes: https://cookpad.com/search/%E8%80%81%E5%B9%B2%E5%AA%BD?order=date&page=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "老\tロウ\t老\t接頭詞-名詞接続\t\t\n",
      "干\tヒ\t干る\t動詞-自立\t一段\t連用形\n",
      "媽\t媽\t媽\t名詞-一般\t\t\n",
      "EOS\n",
      "\n",
      "老\tロウ\t老\t接頭詞-名詞接続\t\t\n",
      "干\tヒ\t干る\t動詞-自立\t一段\t連用形\n",
      "媽\t媽\t媽\t名詞-一般\t\t\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MECAB_TAGGER= MeCab.Tagger(\"-Ochasen\")\n",
    "print( MECAB_TAGGER.parse(\"老干媽\") )\n",
    "print( MECAB_TAGGER.parse(\"老干媽\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ザー\tザー\tザー\t名詞-一般\t\t\n",
      "菜\tサイ\t菜\t名詞-一般\t\t\n",
      "は\tハ\tは\t助詞-係助詞\t\t\n",
      "粗い\tアライ\t粗い\t形容詞-自立\t形容詞・アウオ段\t基本形\n",
      "みじん切り\tミジンギリ\tみじん切り\t名詞-一般\t\t\n",
      "に\tニ\tに\t助詞-格助詞-一般\t\t\n",
      "する\tスル\tする\t動詞-自立\tサ変・スル\t基本形\n",
      "。\t。\t。\t記号-句点\t\t\n",
      "ボール\tボール\tボール\t名詞-一般\t\t\n",
      "に\tニ\tに\t助詞-格助詞-一般\t\t\n",
      "豚肉\tブタニク\t豚肉\t名詞-一般\t\t\n",
      "、\t、\t、\t記号-読点\t\t\n",
      "たね\tタネ\tたね\t名詞-固有名詞-人名-名\t\t\n",
      "用\tヨウ\t用\t名詞-接尾-一般\t\t\n",
      "の\tノ\tの\t助詞-連体化\t\t\n",
      "材料\tザイリョウ\t材料\t名詞-一般\t\t\n",
      "、\t、\t、\t記号-読点\t\t\n",
      "ザー\tザー\tザー\t名詞-一般\t\t\n",
      "菜\tサイ\t菜\t名詞-一般\t\t\n",
      "を\tヲ\tを\t助詞-格助詞-一般\t\t\n",
      "入れ\tイレ\t入れる\t動詞-自立\t一段\t連用形\n",
      "、\t、\t、\t記号-読点\t\t\n",
      "全体\tゼンタイ\t全体\t名詞-副詞可能\t\t\n",
      "に\tニ\tに\t助詞-格助詞-一般\t\t\n",
      "粘り\tネバリ\t粘る\t動詞-自立\t五段・ラ行\t連用形\n",
      "が\tガ\tが\t助詞-格助詞-一般\t\t\n",
      "出る\tデル\t出る\t動詞-自立\t一段\t基本形\n",
      "まで\tマデ\tまで\t助詞-副助詞\t\t\n",
      "手\tテ\t手\t名詞-一般\t\t\n",
      "で\tデ\tで\t助詞-格助詞-一般\t\t\n",
      "練り\tネリ\t練る\t動詞-自立\t五段・ラ行\t連用形\n",
      "混ぜ\tマゼ\t混ぜる\t動詞-自立\t一段\t連用形\n",
      "、\t、\t、\t記号-読点\t\t\n",
      "たね\tタネ\tたね\t名詞-固有名詞-人名-名\t\t\n",
      "を\tヲ\tを\t助詞-格助詞-一般\t\t\n",
      "作る\tツクル\t作る\t動詞-自立\t五段・ラ行\t基本形\n",
      "。\t。\t。\t記号-句点\t\t\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( MECAB_TAGGER.parse(\n",
    "    \"ザー菜は粗いみじん切りにする。ボールに豚肉、たね用の材料、ザー菜を入れ、全体に粘りが出るまで手で練り混ぜ、たねを作る。\"\n",
    ") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the signs which will be eliminated because probably they do not affect the model performance.\n",
    "\n",
    "**If you have your own dictionary, you can set it here (the dict file is assumed to be set on /work/data/text/YOUR_DICT).** <br>\n",
    "e.g.,) In Cookpad, we have our dictionary which is specialized in recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SIGNS = \"，．・：；？！゛゜´｀¨＾￣＿ヽヾ〃仝〆〇‐／∥｜…‥‘’“”〔〕［］｛｝〈〉《》「」『』【】\\\n",
    "        ＋－±×÷＝≠＜＞≦≧∞∴♂♀°′″℃￥＄￠￡％＃＆＊＠§☆★○●◎◇字◆□■△▲▽▼※〒→←↑↓〓\\\n",
    "       ∈∋⊆⊇⊂⊃∪∩∧∨￢⇒⇔∀∃∠⊥⌒∂∇≡≒≪≫√∽∝∵∫∬Å‰♯♭♪†‡¶◯';♥♡♫❤✿。◆◇♢♦❖∮彡☺～α✾✣⁂/*\"\n",
    "\n",
    "# MECAB_USER_DIC_PATH = os.path.join('..', 'data', 'text', YOUR OWN DICT)\n",
    "# MECAB_TAGGER= MeCab.Tagger(\"-Ochasen -u %(MECAB_USER_DIC_PATH)s\" % globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "老干媽\t\t\t名詞-一般\t\t\n",
      "EOS\n",
      "\n",
      "老干媽\t\t\t名詞-一般\t\t\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( MECAB_TAGGER.parse(\"老干媽\") )\n",
    "print( MECAB_TAGGER.parse(\"老干媽\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check morphological analysis using a test sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ザー菜\t\t\t名詞-一般\t\t\n",
      "は\tハ\tは\t助詞-係助詞\t\t\n",
      "粗い\tアライ\t粗い\t形容詞-自立\t形容詞・アウオ段\t基本形\n",
      "みじん切り\t\t\t名詞-一般\t\t\n",
      "に\tニ\tに\t助詞-格助詞-一般\t\t\n",
      "する\tスル\tする\t動詞-自立\tサ変・スル\t基本形\n",
      "。\t。\t。\t記号-句点\t\t\n",
      "ボール\t\t\t名詞-一般\t\t\n",
      "に\tニ\tに\t助詞-格助詞-一般\t\t\n",
      "豚肉\tブタニク\t豚肉\t名詞-一般\t\t\n",
      "、\t、\t、\t記号-読点\t\t\n",
      "たね\t\t\t名詞-一般\t\t\n",
      "用\tヨウ\t用\t名詞-接尾-一般\t\t\n",
      "の\tノ\tの\t助詞-連体化\t\t\n",
      "材料\tザイリョウ\t材料\t名詞-一般\t\t\n",
      "、\t、\t、\t記号-読点\t\t\n",
      "ザー菜\t\t\t名詞-一般\t\t\n",
      "を\tヲ\tを\t助詞-格助詞-一般\t\t\n",
      "入れ\tイレ\t入れる\t動詞-自立\t一段\t連用形\n",
      "、\t、\t、\t記号-読点\t\t\n",
      "全体\tゼンタイ\t全体\t名詞-副詞可能\t\t\n",
      "に\tニ\tに\t助詞-格助詞-一般\t\t\n",
      "粘り\tネバリ\t粘る\t動詞-自立\t五段・ラ行\t連用形\n",
      "が\tガ\tが\t助詞-格助詞-一般\t\t\n",
      "出る\tデル\t出る\t動詞-自立\t一段\t基本形\n",
      "まで\tマデ\tまで\t助詞-副助詞\t\t\n",
      "手\tテ\t手\t名詞-一般\t\t\n",
      "で\tデ\tで\t助詞-格助詞-一般\t\t\n",
      "練り\tネリ\t練る\t動詞-自立\t五段・ラ行\t連用形\n",
      "混ぜ\tマゼ\t混ぜる\t動詞-自立\t一段\t連用形\n",
      "、\t、\t、\t記号-読点\t\t\n",
      "たね\t\t\t名詞-一般\t\t\n",
      "を\tヲ\tを\t助詞-格助詞-一般\t\t\n",
      "作る\t\t\t名詞-一般\t\t\n",
      "。\t。\t。\t記号-句点\t\t\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( MECAB_TAGGER.parse(\n",
    "    \"ザー菜は粗いみじん切りにする。ボールに豚肉、たね用の材料、ザー菜を入れ、全体に粘りが出るまで手で練り混ぜ、たねを作る。\"\n",
    ") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your own sentence!\n",
    "\n",
    "e.g.,) 今日はとても良い天気なので、公園で論文を読もう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日\t\t\t名詞-一般\t\t\n",
      "は\tハ\tは\t助詞-係助詞\t\t\n",
      "とても\t\t\t名詞-一般\t\t\n",
      "良い\tヨイ\t良い\t形容詞-自立\t形容詞・アウオ段\t基本形\n",
      "天気\tテンキ\t天気\t名詞-一般\t\t\n",
      "な\tナ\tだ\t助動詞\t特殊・ダ\t体言接続\n",
      "ので\tノデ\tので\t助詞-接続助詞\t\t\n",
      "、\t、\t、\t記号-読点\t\t\n",
      "公園\t\t\t名詞-一般\t\t\n",
      "で\tデ\tで\t助詞-格助詞-一般\t\t\n",
      "論文\tロンブン\t論文\t名詞-一般\t\t\n",
      "を\tヲ\tを\t助詞-格助詞-一般\t\t\n",
      "読も\tヨモ\t読む\t動詞-自立\t五段・マ行\t未然ウ接続\n",
      "う\tウ\tう\t助動詞\t不変化型\t基本形\n",
      "。\t。\t。\t記号-句点\t\t\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( MECAB_TAGGER.parse(\"今日はとても良い天気なので、公園で論文を読もう。\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deifine some functions for modeling.\n",
    "\n",
    "Return the recipe text for each data and for each p(ositive) or n(egative) label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recipes_for_label(label):\n",
    "    with open('../data/text/recipes/%(label)s_positive.pkl' % locals(), 'rb') as f:\n",
    "        p = pickle.load(f)\n",
    "    \n",
    "    with open('../data/text/recipes/%(label)s_negative.pkl' % locals(), 'rb') as f:\n",
    "        n = pickle.load(f)\n",
    "        \n",
    "    return p, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the function. Here we use sample data (this is just a simple and virtual data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos, neg = recipes_for_label(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ingredients': ['豚肉', 'にんにく', 'にら'],\n",
       " 'steps': ['たねを作る', '皮に包む', '美味しく焼く'],\n",
       " 'title': '激ウマ！焼き餃子'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_idx = 1\n",
    "pos[pos_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ingredients': ['パスタ', '卵', 'ベーコン', '生クリーム'],\n",
       " 'steps': ['パスタを茹でる', 'ソースをいい感じに作っておく', '和える'],\n",
       " 'title': 'オシャレパスタ'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_idx = 1\n",
    "neg[neg_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing functions: eliminate the signs and extract nouns.\n",
    "\n",
    "In this analysis we ONLY use noun as input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _clean(in_str):\n",
    "    return re.sub(r'\\d+', ' ', re.sub(r\"[%(SIGNS)s]+\" % globals(), ' ', in_str))\n",
    "\n",
    "def _extract_nouns(in_str):\n",
    "    words = [ws.split('\\t') for ws in MECAB_TAGGER.parse(in_str).split('\\n')[:-2]]\n",
    "    nouns = [w[0] for w in words if w[3].startswith('名')] # use only noun for input features\n",
    "    return ' '.join(nouns)\n",
    "\n",
    "def clean_and_extract_nouns(in_str):\n",
    "    return _extract_nouns(_clean(in_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "キラッ 流星にまたがって\n",
      "キラッ 流星\n"
     ]
    }
   ],
   "source": [
    "text = \"キラッ☆流星にまたがって\"\n",
    "print( _clean(text) )\n",
    "print( _extract_nouns(_clean(text)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing: concat preprocessed {title, ingredients, steps} texts and return RECIPEID: TEXT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_recipes(recipes):\n",
    "    def flatten_recipe(recipe):\n",
    "        return ' '.join([recipe['title']] + recipe['ingredients'] + recipe['steps'])\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        processed_recipes = executor.map(clean_and_extract_nouns, map(flatten_recipe, recipes.values()))\n",
    "        result = {rid: recipe for rid, recipe in zip(recipes.keys(), processed_recipes)}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '激 ウマ 焼き 餃子 豚肉 にんにく にら たね 作る 皮 包む 美味しく 焼く',\n",
       " 2: '最高 焼き 餃子 豚肉 にんにく にら 春菊 たね 作る 皮 包む 美味しく 焼く'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_recipes(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'オシャレ パスタ パスタ 卵 ベーコン 生クリーム パスタ 茹でる ソース いい 感じ 和える',\n",
       " 2: 'センス パスタ パスタ パンチェッタ オリーブオイル パスタ 茹でる ソース いい 感じ 和える'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_recipes(neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing: compute tfidf from input and vectorize input.\n",
    "\n",
    "tfidf ( https://en.wikipedia.org/wiki/Tf%E2%80%93idf ) is one of the most useful text features for machine learning.\n",
    "- tf is term frequency: number of occurences of each word (counting is done sentence by sentence)\n",
    "- idf is inverse document frequency: weighting each word in accordance with the occurence times in all documents <br>\n",
    "  $$ \\text{(weight for the target word) ~} \\log \\frac{ \\text{(# of all documents)} }{ \\text{(# of documents including the word)} } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_idx_features(bowed_recipes):\n",
    "    recipe_ids, features = zip(*bowed_recipes.items())\n",
    "    return np.array(list(recipe_ids)), np.array(list(features))\n",
    "\n",
    "def tfidf_vectorizer(bowed_recipes):\n",
    "    v = TfidfVectorizer()\n",
    "    _, r = _get_idx_features(bowed_recipes)\n",
    "    return v.fit(r)\n",
    "\n",
    "def vectorize_recipes(bowed_recipes, vectorizer):\n",
    "    recipe_ids, r = _get_idx_features(bowed_recipes)\n",
    "    return np.array(list(recipe_ids)), vectorizer.transform(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n",
      "[[ 0.          1.          0.          0.        ]\n",
      " [ 0.53404633  0.37997836  0.53404633  0.53404633]]\n"
     ]
    }
   ],
   "source": [
    "text_dict = {\n",
    "    1: \"うらみます うらみます うらみます\"\n",
    "    , 2: \"うらみます あんた こと 死ぬまで\"    \n",
    "}\n",
    "\n",
    "text_tfidf = tfidf_vectorizer(text_dict)\n",
    "recipe_ids,text_vector = vectorize_recipes(text_dict, text_tfidf)\n",
    "\n",
    "print(recipe_ids)\n",
    "print(text_vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing: training data generator.\n",
    "\n",
    "For simplicity, we treat data as numpy array; we simply stack vectorized preprocessed text features for X and labels for y.\n",
    "\n",
    "Scipy sparse matrix enables us to treat sparse ( most of entries is zero ) data efiiciently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_train_data(label):\n",
    "    p_recipes, n_recipes = recipes_for_label(label)\n",
    "\n",
    "    p_recipes_tokenized = tokenize_recipes(p_recipes)\n",
    "    n_recipes_tokenized = tokenize_recipes(n_recipes)\n",
    "\n",
    "    all_recipes_tokenized = {**p_recipes_tokenized, **n_recipes_tokenized}\n",
    "    \n",
    "    v = tfidf_vectorizer(all_recipes_tokenized)\n",
    "\n",
    "    p_recipe_ids, p_recipes_vectorized = vectorize_recipes(p_recipes_tokenized, v)\n",
    "    n_recipe_ids, n_recipes_vectorized = vectorize_recipes(n_recipes_tokenized, v)\n",
    "\n",
    "    p_labels = np.empty(p_recipes_vectorized.shape[0])\n",
    "    p_labels.fill(0)\n",
    "    n_labels = np.empty(n_recipes_vectorized.shape[0])\n",
    "    n_labels.fill(1)\n",
    "\n",
    "    recipe_ids = np.concatenate([p_recipe_ids, n_recipe_ids])\n",
    "    y = np.concatenate([p_labels, n_labels])\n",
    "    X = vstack([p_recipes_vectorized, n_recipes_vectorized])\n",
    "\n",
    "    return (recipe_ids, X.toarray(), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Counstruct a random forest model\n",
    "\n",
    "For starters, we try to construct a model of Random Forest ( https://en.wikipedia.org/wiki/Random_forest ) .\n",
    "\n",
    "Random forest is an ensemble method of decision trees; it is powerful and stable (very nice model as a 1st choice). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare your data and set the target recipe.\n",
    "\n",
    "You can use \"sample\" labbel if you don't have any data; however the result is NOT meaningful since the data size is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos, neg = recipes_for_label(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# This could take minutes\n",
    "recipe_ids, X, y = generate_train_data(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipe_ids[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into {train, test}; here we use 10% data as test.\n",
    "\n",
    "random_state ensures the reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipe_ids_train, recipe_ids_test, X_train, X_test, y_train, y_test = train_test_split(\n",
    "    recipe_ids, X, y, test_size=0.2, random_state=1729\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(y_train), sum(y_train))\n",
    "print(len(y_test), sum(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model.\n",
    "\n",
    "Though RandomForestClassifier has variosu parameters ( http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html ), we use default parameters except for n_estimators which is the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = RandomForestClassifier(random_state=1729, n_estimators=50)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the model peformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test)\n",
    "print(precision_score(y_test, y_pred_test))\n",
    "print(recall_score(y_test, y_pred_test))\n",
    "print(f1_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the wrong predictions. **NOTE that we define 0:positive, 1:negative**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: sweets <br>\n",
    "Prediction: not sweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong_recipe_ids = np.where( ( y_test == 0 ) & (y_pred_test  == 1 ) )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong_recipe_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipe_ids_test[wrong_recipe_ids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos[recipe_ids_test[wrong_recipe_ids[0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: not sweets <br>\n",
    "Prediction: sweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong_recipe_ids = np.where( ( y_test == 1 ) & (y_pred_test  == 0 ) )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong_recipe_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipe_ids_test[wrong_recipe_ids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg[recipe_ids_test[wrong_recipe_ids[0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construct a Xgboost model\n",
    "\n",
    "Let's try to use Xgboost ( paper: https://arxiv.org/abs/1603.02754 , GitHub: https://github.com/dmlc/xgboost ) which is known as the most useful model for structured data, especially in kaggle ( https://www.kaggle.com/ ) .\n",
    "Recently LightGBM ( GitHub: https://github.com/Microsoft/LightGBM ) is also said to be powerful.\n",
    "\n",
    "Xgboost is similar to Random Forest; this is an emsenble method (and in most case weak learners are decision trees.)\n",
    "\n",
    "Although Random Forest uses simple average for summarizing weak learners' results, Xgboost (precisely, Gradient Boosting) changes weights for each tree to minimize a objective function by computing the derivative of it.\n",
    "\n",
    "For strucutered dataset, **DON'T THINK, USE Xgboost (or LightGBM)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use xgboost.skleran.XGBClassifier; fortunately, we can use sklearn-like API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Xgboost parametes ( http://xgboost.readthedocs.io/en/latest/python/python_api.html )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "        objective= 'binary:logistic'\n",
    "        , learning_rate =0.4\n",
    "        , n_estimators=50\n",
    "        , max_depth=10\n",
    "        , min_child_weight=1.2\n",
    "        , gamma=0.4\n",
    "        , subsample=0.7\n",
    "        , colsample_bytree=0.7\n",
    "        , scale_pos_weight=1\n",
    "        , seed=1729\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_test = model.predict(X_test)\n",
    "\n",
    "print(precision_score(y_test, pred_test))\n",
    "print(recall_score(y_test, pred_test))\n",
    "print(f1_score(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Additional task) Try to find good parameters.\n",
    "\n",
    "Many machine learning models have various hyper parameters which are parameters human must give (not given through the training process).\n",
    "\n",
    "Because model performances are very depending on these parameters, we need to tune them to get better result.\n",
    "\n",
    "Unfortunately, the dependencies of the performances on hyper parameters are highly non-linear; all we can do is to use grid search or bayesian optimization.\n",
    "\n",
    "Here we use the grid search to find better parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!! Skip this cell because it will take too much time !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# param_test = {\n",
    "#     'learning_rate': [0.2,0.3,0.4]\n",
    "#     , 'n_estimators': [125,150,175]\n",
    "#     , 'max_depth': [10,15,20]\n",
    "# }\n",
    "# gsearch = GridSearchCV(estimator = XGBClassifier( \n",
    "#     min_child_weight=1\n",
    "#     , gamma=0\n",
    "#     , subsample=0.8\n",
    "#     , colsample_bytree=0.8\n",
    "#     , objective= 'binary:logistic'\n",
    "#     , nthread=4\n",
    "#     , scale_pos_weight=1\n",
    "#     , seed=1729\n",
    "#     ), \n",
    "#     param_grid = param_test, scoring='roc_auc',n_jobs=1,iid=False, cv=5\n",
    "# )\n",
    "\n",
    "# gsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Use the original interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# import xgboost as xgb\n",
    "\n",
    "# dtrain = xgb.DMatrix(X_train, label=y_train) # DMatrix is a kind of data structure\n",
    "\n",
    "# params = {\n",
    "#         'objective': 'binary:logistic'\n",
    "#         , 'eta' : 0.2\n",
    "#         , 'max_depth': 10\n",
    "#         , 'min_child_weight': 1.2\n",
    "#         , 'gamma': 0.4\n",
    "#         , 'subsample': 0.7\n",
    "#         , 'colsample_bytree': 0.7\n",
    "#         , 'lambda': 1\n",
    "#         , 'alpha': 0\n",
    "#         , 'seed': 1729\n",
    "# }\n",
    "\n",
    "# model = xgb.train(dtrain=dtrain, params=params, num_boost_round=175)\n",
    "\n",
    "# dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# prediction_raw = model.predict(dtest) # Model predicts probabilities of \"True\" as [0,1] values.\n",
    "\n",
    "# thld = 0.5\n",
    "# pred_test = [1 if elem > thld else 0 for elem in prediction_raw]\n",
    "\n",
    "# print(precision_score(y_test, pred_test))\n",
    "# print(recall_score(y_test, pred_test))\n",
    "# print(f1_score(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How can we improve the model?\n",
    "\n",
    "- Data size is the most important factor\n",
    "- Data cleansing\n",
    "- Mofication of the model\n",
    "- Rethinking problem settings\n",
    "- ...\n",
    "\n",
    "**It requires your creativity!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Please describe your ideas to improve the model performance\n",
    "- Please implement your ideas and check the result\n",
    "- Can you explain how the morphological analysis works?\n",
    "- Can you explain tf-idf in detail?\n",
    "- Can you explain what is the ensemble method?\n",
    "- Can you explain the algorithm of the gradient boosting? <br>\n",
    "  (especially the difference from Random Forest)\n",
    "- Can you explain the meanings of the parameters of Random Forest or Xgboost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
